import torch
from torch.autograd import Function
import triton
import triton.language as tl
from torch.amp import custom_fwd, custom_bwd
import math


def _grid(numel: int, bs: int) -> tuple:
    return (triton.cdiv(numel, bs),)


@triton.jit
def _idx(i, n: int, c: int, h: int, w: int):
    ni = i // (c * h * w)
    ci = (i // (h * w)) % c
    hi = (i // w) % h
    wi = i % w
    m = i < (n * c * h * w)
    return ni, ci, hi, wi, m


@triton.jit
def nka_fwd(
    x_ptr, w_ptr, o_ptr,
    n, ic, h, w, kn, pad, groups,
    BS: tl.constexpr,
    CT: tl.constexpr, AT: tl.constexpr
):
    """Narrow-Kernel Aggregation Forward Pass
    
    Args:
        x_ptr: Input feature pointer
        w_ptr: Dynamic kernel weights from WKP
        o_ptr: Output pointer
        n: batch size
        ic: input channels
        h, w: spatial dimensions
        kn: narrow kernel size (K_n in paper)
        pad: padding size
        groups: number of groups for grouped convolution
    """
    pid = tl.program_id(0)
    start = pid * BS
    offs = start + tl.arange(0, BS)

    ni, ci, hi, wi, m = _idx(offs, n, ic, h, w)
    val = tl.zeros((BS,), dtype=AT)

    # Apply narrow-kernel convolution with dynamic weights
    for kh in range(kn):
        hin = hi - pad + kh
        hb = (hin >= 0) & (hin < h)

        x_off = ((ni * ic + ci) * h + hin) * w + wi
        w_off = ((ni * groups + ci % groups) * kn + kh) * h * w + hi * w + wi

        x_val = tl.load(x_ptr + x_off, mask=m & hb, other=0.0).to(CT)
        w_val = tl.load(w_ptr + w_off, mask=m, other=0.0).to(CT)
        val += tl.where(hb & m, x_val * w_val, 0.0).to(AT)

    tl.store(o_ptr + offs, val.to(CT), mask=m)


@triton.jit
def nka_bwd_x(
    go_ptr, w_ptr, gi_ptr,
    n, ic, h, w, kn, pad, groups,
    BS: tl.constexpr,
    CT: tl.constexpr, AT: tl.constexpr
):
    """Narrow-Kernel Aggregation Backward Pass for Input"""
    pid = tl.program_id(0)
    start = pid * BS
    offs = start + tl.arange(0, BS)

    ni, ci, hi, wi, m = _idx(offs, n, ic, h, w)
    val = tl.zeros((BS,), dtype=AT)

    for kh in range(kn):
        ho = hi + pad - kh
        hb = (ho >= 0) & (ho < h)

        go_off = ((ni * ic + ci) * h + ho) * w + wi
        w_off = ((ni * groups + ci % groups) * kn + kh) * h * w + ho * w + wi

        go_val = tl.load(go_ptr + go_off, mask=m & hb, other=0.0).to(CT)
        w_val = tl.load(w_ptr + w_off, mask=m, other=0.0).to(CT)
        val += tl.where(hb & m, go_val * w_val, 0.0).to(AT)

    tl.store(gi_ptr + offs, val.to(CT), mask=m)


@triton.jit
def nka_bwd_w(
    go_ptr, x_ptr, gw_ptr,
    n, groups, h, w, ic, kn, pad,
    BS: tl.constexpr,
    CT: tl.constexpr, AT: tl.constexpr
):
    """Narrow-Kernel Aggregation Backward Pass for Dynamic Kernels"""
    pid = tl.program_id(0)
    start = pid * BS
    offs = start + tl.arange(0, BS)

    ni, ci, hi, wi, m = _idx(offs, n, groups, h, w)

    for kh in range(kn):
        hin = hi - pad + kh
        hb = (hin >= 0) & (hin < h)
        w_off = ((ni * groups + ci) * kn + kh) * h * w + hi * w + wi

        val = tl.zeros((BS,), dtype=AT)
        steps = (ic - ci + groups - 1) // groups
        for s in range(tl.max(steps, axis=0)):
            cc = ci + s * groups
            cm = (cc < ic) & m & hb

            x_off = ((ni * ic + cc) * h + hin) * w + wi
            go_off = ((ni * ic + cc) * h + hi) * w + wi

            x_val = tl.load(x_ptr + x_off, mask=cm, other=0.0).to(CT)
            go_val = tl.load(go_ptr + go_off, mask=cm, other=0.0).to(CT)
            val += tl.where(cm, x_val * go_val, 0.0).to(AT)

        tl.store(gw_ptr + w_off, val.to(CT), mask=m)


class NarrowKernelAggregation(Function):
    """
    Narrow-Kernel Aggregation (NKA) Function
    
    Performs content-adaptive feature aggregation using position-specific 
    dynamic kernels generated by Wide-Kernel Perception (WKP).
    
    Following the "scan wide, focus narrow" principle from WNnet paper.
    """
    @staticmethod
    @custom_fwd(device_type='cuda')
    def forward(ctx, x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input features [B, C, H, W]
            w: Dynamic kernels from WKP [B, groups, K_n, H, W]
        
        Returns:
            o: Aggregated features [B, C, H, W]
        """
        # K_n: narrow kernel size for focused aggregation
        kn = int(w.shape[2])
        pad = (kn - 1) // 2
        ctx.kn, ctx.pad = kn, pad
        n, ic, h, width = x.shape
        groups = w.shape[1]
        o = torch.empty(n, ic, h, width, device=x.device, dtype=x.dtype)
        numel = o.numel()

        x = x.contiguous()
        w = w.contiguous()

        grid = lambda meta: _grid(numel, meta["BS"])

        ct = tl.float16 if x.dtype == torch.float16 else (tl.float32 if x.dtype == torch.float32 else tl.float64)
        at = tl.float32 if x.dtype == torch.float16 else ct

        nka_fwd[grid](x, w, o, n, ic, h, width, kn, pad, groups, BS=1024, CT=ct, AT=at)

        ctx.save_for_backward(x, w)
        ctx.ct, ctx.at = ct, at
        return o

    @staticmethod
    @custom_bwd(device_type='cuda')
    def backward(ctx, go: torch.Tensor) -> tuple:
        kn, pad = ctx.kn, ctx.pad
        x, w = ctx.saved_tensors
        n, ic, h, width = x.shape
        groups = w.shape[1]

        go = go.contiguous()
        gx = gw = None
        ct, at = ctx.ct, ctx.at

        if ctx.needs_input_grad[0]:
            gx = torch.empty_like(x)
            numel = gx.numel()
            nka_bwd_x[lambda meta: _grid(numel, meta["BS"])](go, w, gx, n, ic, h, width, kn, pad, groups, BS=1024, CT=ct, AT=at)

        if ctx.needs_input_grad[1]:
            gw = torch.empty_like(w)
            numel = gw.numel() // w.shape[2]
            nka_bwd_w[lambda meta: _grid(numel, meta["BS"])](go, x, gw, n, groups, h, width, ic, kn, pad, BS=1024, CT=ct, AT=at)

        return gx, gw, None, None


class NKA(torch.nn.Module):
    """
    Narrow-Kernel Aggregation Module
    
    Applies position-specific dynamic convolutions for adaptive feature fusion.
    Works in conjunction with Wide-Kernel Perception (WKP) to implement the
    "scan wide, focus narrow" principle.
    
    Usage:
        nka = NKA()
        output = nka(features, dynamic_kernels)
    """
    def forward(self, x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input features [B, C, H, W]
            w: Dynamic kernels from WKP [B, groups, K_n, H, W]
            
        Returns:
            Aggregated features [B, C, H, W]
        """
        return NarrowKernelAggregation.apply(x, w)  # type: ignore